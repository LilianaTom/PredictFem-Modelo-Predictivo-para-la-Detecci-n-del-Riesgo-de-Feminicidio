{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a090be60-733d-4a8a-9144-2b3a11077761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 906, number of negative: 1115\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000215 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 559\n",
      "[LightGBM] [Info] Number of data points in the train set: 2021, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448293 -> initscore=-0.207570\n",
      "[LightGBM] [Info] Start training from score -0.207570\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 724, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000371 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 554\n",
      "[LightGBM] [Info] Number of data points in the train set: 1616, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448020 -> initscore=-0.208675\n",
      "[LightGBM] [Info] Start training from score -0.208675\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000308 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 551\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000147 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 558\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000148 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 552\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000330 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 553\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:547: FitFailedWarning: \n",
      "90 fits failed out of a total of 500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "90 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\klgt1\\AppData\\Local\\Temp\\ipykernel_17204\\1834542126.py\", line 49, in fit\n",
      "  File \"C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py\", line 669, in fit\n",
      "    return super().fit(X, y_encoded, sample_weight)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_stacking.py\", line 216, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 67, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py\", line 1863, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py\", line 1792, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py\", line 129, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_base.py\", line 36, in _fit_single_estimator\n",
      "    estimator.fit(X, y)\n",
      "  File \"C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1467, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_split' parameter of ExtraTreesClassifier must be an int in the range [2, inf) or a float in the range (0.0, 1.0]. Got 1 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.70296402 0.67441654 0.68714493 0.69665216 0.68905944 0.70297054\n",
      " 0.69039015 0.69442084 0.70196753 0.6935813  0.69463326 0.70167553\n",
      "        nan 0.68670497 0.6860535  0.70049492 0.70096853 0.69048291\n",
      " 0.69969334 0.69003006 0.68796906 0.69348238 0.70178072        nan\n",
      " 0.69720545 0.69207803 0.69536327 0.69867428        nan 0.69497454\n",
      " 0.68591242 0.70280806 0.68634585        nan        nan 0.69182241\n",
      " 0.69666647 0.67997636        nan 0.69708642        nan 0.69800681\n",
      " 0.69471917 0.69094072 0.69395114 0.69749745        nan 0.70546598\n",
      "        nan 0.69997775 0.69782282 0.68977453 0.69980646 0.69548367\n",
      "        nan 0.69069987 0.70063539 0.68655613 0.69239471 0.69173304\n",
      "        nan 0.68615553 0.68919804        nan 0.68026356 0.69896081\n",
      " 0.69392328 0.6956271  0.6922075  0.69525967 0.68442247 0.67563761\n",
      " 0.69651561 0.68516919        nan 0.68745565 0.69877868 0.69984774\n",
      " 0.69049439 0.69240487 0.69436586 0.70102556 0.69932639 0.68663816\n",
      "        nan 0.69936749        nan 0.69832474 0.69074733 0.70185955\n",
      " 0.7007944         nan 0.68882955 0.68615862        nan 0.70221207\n",
      " 0.69745518 0.69463154        nan 0.68783355]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1123, number of negative: 1404\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000407 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 556\n",
      "[LightGBM] [Info] Number of data points in the train set: 2527, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.444400 -> initscore=-0.223322\n",
      "[LightGBM] [Info] Start training from score -0.223322\n",
      "[LightGBM] [Info] Number of positive: 898, number of negative: 1123\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002565 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 562\n",
      "[LightGBM] [Info] Number of data points in the train set: 2021, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.444334 -> initscore=-0.223589\n",
      "[LightGBM] [Info] Start training from score -0.223589\n",
      "[LightGBM] [Info] Number of positive: 898, number of negative: 1123\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000572 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 564\n",
      "[LightGBM] [Info] Number of data points in the train set: 2021, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.444334 -> initscore=-0.223589\n",
      "[LightGBM] [Info] Start training from score -0.223589\n",
      "[LightGBM] [Info] Number of positive: 898, number of negative: 1124\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000292 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 560\n",
      "[LightGBM] [Info] Number of data points in the train set: 2022, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.444115 -> initscore=-0.224479\n",
      "[LightGBM] [Info] Start training from score -0.224479\n",
      "[LightGBM] [Info] Number of positive: 899, number of negative: 1123\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000246 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 563\n",
      "[LightGBM] [Info] Number of data points in the train set: 2022, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.444609 -> initscore=-0.222476\n",
      "[LightGBM] [Info] Start training from score -0.222476\n",
      "[LightGBM] [Info] Number of positive: 899, number of negative: 1123\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000214 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 558\n",
      "[LightGBM] [Info] Number of data points in the train set: 2022, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.444609 -> initscore=-0.222476\n",
      "[LightGBM] [Info] Start training from score -0.222476\n",
      "Mejores hiperparámetros:\n",
      "{'bootstrap': False, 'colsample_bytree': 0.8742907123527768, 'criterion': 'entropy', 'gamma': 4.922452675963152, 'learning_rate': 0.43496590603574514, 'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 5, 'min_samples_split': 3, 'n_estimators': 20, 'num_leaves': 19, 'subsample': 0.8048639806168911}\n",
      "Mejor score:  0.7054659812939423\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import randint, uniform\n",
    "import pandas as pd\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, \n",
    "                 max_features, bootstrap, learning_rate, subsample, colsample_bytree, num_leaves, gamma):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.bootstrap = bootstrap\n",
    "        self.random_state = 42\n",
    "        self.stacking_model = None  \n",
    "        self.predictions = []\n",
    "        self.learning_rate = learning_rate  \n",
    "        self.subsample = subsample  \n",
    "        self.colsample_bytree = colsample_bytree  \n",
    "        self.num_leaves = num_leaves\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def fit(self, X, y):        \n",
    "        n_estimators_per_model = self.n_estimators // 3\n",
    "        # Definir el clasificador de nivel 1\n",
    "        estimators = [\n",
    "            ('xgb', XGBClassifier(n_estimators=n_estimators_per_model, max_depth=self.max_depth, random_state=self.random_state,\n",
    "                                  learning_rate=self.learning_rate, subsample=self.subsample, colsample_bytree=self.colsample_bytree,\n",
    "                                  gamma=self.gamma)),\n",
    "            ('lgbm', LGBMClassifier(n_estimators=n_estimators_per_model, max_depth=self.max_depth, random_state=self.random_state,\n",
    "                                    learning_rate=self.learning_rate, subsample=self.subsample, colsample_bytree=self.colsample_bytree,\n",
    "                                    num_leaves=self.num_leaves)),\n",
    "            ('extra_trees', ExtraTreesClassifier(n_estimators=n_estimators_per_model, criterion=self.criterion,\n",
    "                                                 max_depth=self.max_depth, min_samples_split=self.min_samples_split,\n",
    "                                                 min_samples_leaf=self.min_samples_leaf, max_features=self.max_features,\n",
    "                                                 bootstrap=self.bootstrap, random_state=self.random_state))]\n",
    "            \n",
    "        # Definir el clasificador de nivel 2 (modelo base)\n",
    "        rf_base = RandomForestClassifier(n_estimators=self.n_estimators, random_state=self.random_state)\n",
    "        \n",
    "        # Crear el modelo de stacking\n",
    "        self.stacking_model = StackingClassifier(estimators=estimators, final_estimator=rf_base)\n",
    "        self.stacking_model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.stacking_model.predict(X)\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'criterion': self.criterion,\n",
    "            'max_depth': self.max_depth,\n",
    "            'min_samples_split': self.min_samples_split,\n",
    "            'min_samples_leaf': self.min_samples_leaf,\n",
    "            'max_features': self.max_features,\n",
    "            'bootstrap': self.bootstrap,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'subsample': self.subsample,\n",
    "            'colsample_bytree': self.colsample_bytree,\n",
    "            'num_leaves': self.num_leaves,\n",
    "            'gamma': self.gamma\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n",
    "\n",
    "class RandomForestClassifierWrapper:\n",
    "    def __init__(self, X, y, n_estimators, criterion, max_depth, min_samples_split,\n",
    "                 min_samples_leaf, max_features, bootstrap, learning_rate, subsample, colsample_bytree, num_leaves, gamma):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        self.X_train = self.X_train.values\n",
    "        self.y_train = self.y_train.values\n",
    "        \n",
    "        self.rf = RandomForest(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth,\n",
    "                                min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
    "                                max_features=max_features, bootstrap=bootstrap,\n",
    "                                learning_rate=learning_rate, subsample=subsample,\n",
    "                                colsample_bytree=colsample_bytree, num_leaves=num_leaves, gamma=gamma)\n",
    "        self.estimators = []\n",
    "\n",
    "    def train(self):\n",
    "        self.rf.fit(self.X_train, self.y_train)\n",
    "        \n",
    "    def predict(self):\n",
    "        return self.rf.predict(self.X_test)\n",
    "\n",
    "    def evaluate(self):\n",
    "        y_pred = self.predict()\n",
    "        \n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        precision = precision_score(self.y_test, y_pred, average='macro')\n",
    "        recall = recall_score(self.y_test, y_pred, average='macro')\n",
    "        f1 = f1_score(self.y_test, y_pred, average='macro')\n",
    "        return accuracy, precision, recall, f1\n",
    "\n",
    "def custom_scoring(y_true, y_pred):\n",
    "    return precision_score(y_true, y_pred, average='macro')\n",
    "\n",
    "# Leer datos\n",
    "df = pd.read_excel(\"C:\\\\Users\\\\klgt1\\\\Downloads\\\\dataset_BALANCEADO.xlsx\")\n",
    "X = df.drop('CONDUCTA', axis=1)\n",
    "y = df['CONDUCTA']\n",
    "\n",
    "# Definir el espacio de búsqueda de hiperparámetros\n",
    "param_dist = {\n",
    "    'n_estimators': randint(20, 100),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': randint(3, 20),\n",
    "    'min_samples_split': randint(1, 6),\n",
    "    'min_samples_leaf': randint(1, 6),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'bootstrap': [True, False],\n",
    "    'learning_rate': uniform(0.001, 0.5),\n",
    "    'subsample': uniform(0.5, 0.5),\n",
    "    'colsample_bytree': uniform(0.5, 0.5),\n",
    "    'num_leaves': randint(2, 20),\n",
    "    'gamma': uniform(0, 5)\n",
    "}\n",
    "\n",
    "# Crear instancia del clasificador\n",
    "rf_wrapper = RandomForestClassifierWrapper(X, y, n_estimators=100, criterion='gini', max_depth=10, min_samples_split=2,\n",
    "                                           min_samples_leaf=1, max_features=None, bootstrap=True, learning_rate=0.1,\n",
    "                                           subsample=0.8, colsample_bytree=0.8, num_leaves=31, gamma=0)\n",
    "# Entrenar el modelo\n",
    "rf_wrapper.train()\n",
    "\n",
    "# Crear instancia de RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(rf_wrapper.rf, param_distributions=param_dist, n_iter=100, cv=5, scoring='precision_macro', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Entrenar el modelo\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"Mejores hiperparámetros:\")\n",
    "print(random_search.best_params_)\n",
    "print(\"Mejor score: \", random_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180e3510-e4c9-4e8f-ac96-cc2848b468a4",
   "metadata": {},
   "source": [
    "# Nuestro modelo con búsqueda exhaustiva para selección de parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5f650-f32b-40e6-b4e0-f55728957ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall xgboost\n",
    "!pip install -U xgboost\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, \n",
    "                 max_features, bootstrap, learning_rate, subsample, colsample_bytree, num_leaves, gamma,\n",
    "                 objective='binary', max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                 min_weight_fraction_leaf=0.0, min_child_samples=20, feature_fraction=0.6):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.bootstrap = bootstrap\n",
    "        self.random_state = 42\n",
    "        self.stacking_model = None  \n",
    "        self.predictions = []\n",
    "        self.learning_rate = learning_rate  \n",
    "        self.subsample = subsample  \n",
    "        self.colsample_bytree = colsample_bytree  \n",
    "        self.num_leaves = num_leaves\n",
    "        self.gamma = gamma\n",
    "        self.objective = objective\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.min_child_samples = min_child_samples\n",
    "        self.feature_fraction = feature_fraction\n",
    "\n",
    "    def fit(self, X, y):        \n",
    "        n_estimators_per_model = self.n_estimators // 3\n",
    "        # Definir el clasificador de nivel 1\n",
    "        estimators = [\n",
    "            ('xgb', XGBClassifier(n_estimators=n_estimators_per_model, max_depth=self.max_depth, random_state=self.random_state,\n",
    "                                  learning_rate=self.learning_rate, subsample=self.subsample, colsample_bytree=self.colsample_bytree,\n",
    "                                  gamma=self.gamma, objective=self.objective, max_leaf_nodes=self.max_leaf_nodes,\n",
    "                                  min_impurity_decrease=self.min_impurity_decrease, min_weight_fraction_leaf=self.min_weight_fraction_leaf,\n",
    "                                  min_child_samples=self.min_child_samples, feature_fraction=self.feature_fraction)),\n",
    "            ('lgbm', LGBMClassifier(n_estimators=n_estimators_per_model, max_depth=self.max_depth, random_state=self.random_state,\n",
    "                                    learning_rate=self.learning_rate, subsample=self.subsample, colsample_bytree=self.colsample_bytree,\n",
    "                                    num_leaves=self.num_leaves, max_leaf_nodes=self.max_leaf_nodes,\n",
    "                                    min_child_samples=self.min_child_samples, feature_fraction=self.feature_fraction)),\n",
    "            ('extra_trees', ExtraTreesClassifier(n_estimators=n_estimators_per_model, criterion=self.criterion,\n",
    "                                                 max_depth=self.max_depth, min_samples_split=self.min_samples_split,\n",
    "                                                 min_samples_leaf=self.min_samples_leaf, max_features=self.max_features,\n",
    "                                                 bootstrap=self.bootstrap, random_state=self.random_state))]\n",
    "            \n",
    "        # Definir el clasificador de nivel 2 (modelo base)\n",
    "        rf_base = RandomForestClassifier(n_estimators=self.n_estimators, random_state=self.random_state)\n",
    "        \n",
    "        # Crear el modelo de stacking\n",
    "        self.stacking_model = StackingClassifier(estimators=estimators, final_estimator=rf_base)\n",
    "        self.stacking_model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.stacking_model.predict(X)\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'criterion': self.criterion,\n",
    "            'max_depth': self.max_depth,\n",
    "            'min_samples_split': self.min_samples_split,\n",
    "            'min_samples_leaf': self.min_samples_leaf,\n",
    "            'max_features': self.max_features,\n",
    "            'bootstrap': self.bootstrap,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'subsample': self.subsample,\n",
    "            'colsample_bytree': self.colsample_bytree,\n",
    "            'num_leaves': self.num_leaves,\n",
    "            'gamma': self.gamma,\n",
    "            'objective': self.objective,\n",
    "            'max_leaf_nodes': self.max_leaf_nodes,\n",
    "            'min_impurity_decrease': self.min_impurity_decrease,\n",
    "            'min_weight_fraction_leaf': self.min_weight_fraction_leaf,\n",
    "            'min_child_samples': self.min_child_samples,\n",
    "            'feature_fraction': self.feature_fraction\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n",
    "\n",
    "class RandomForestClassifierWrapper:\n",
    "    def __init__(self, X, y, n_estimators, criterion, max_depth, min_samples_split,\n",
    "                 min_samples_leaf, max_features, bootstrap, learning_rate, subsample, colsample_bytree, num_leaves, gamma,\n",
    "                 objective='binary', max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                 min_weight_fraction_leaf=0.0, min_child_samples=20, feature_fraction=0.6):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        self.X_train = self.X_train.values\n",
    "        self.y_train = self.y_train.values\n",
    "        \n",
    "        self.rf = RandomForest(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth,\n",
    "                                min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
    "                                max_features=max_features, bootstrap=bootstrap,\n",
    "                                learning_rate=learning_rate, subsample=subsample,\n",
    "                                colsample_bytree=colsample_bytree, num_leaves=num_leaves, gamma=gamma,\n",
    "                                objective=objective, max_leaf_nodes=max_leaf_nodes,\n",
    "                                min_impurity_decrease=min_impurity_decrease, min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "                                min_child_samples=min_child_samples, feature_fraction=feature_fraction)\n",
    "        self.estimators = []\n",
    "\n",
    "    def train(self):\n",
    "        self.rf.fit(self.X_train, self.y_train)\n",
    "        \n",
    "    def predict(self):\n",
    "        return self.rf.predict(self.X_test)\n",
    "\n",
    "    def evaluate(self):\n",
    "        y_pred = self.predict()\n",
    "        \n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        precision = precision_score(self.y_test, y_pred, average='macro')\n",
    "        recall = recall_score(self.y_test, y_pred, average='macro')\n",
    "        f1 = f1_score(self.y_test, y_pred, average='macro')\n",
    "        return accuracy, precision, recall, f1\n",
    "\n",
    "# Leer datos\n",
    "df = pd.read_excel(\"C:\\\\Users\\\\klgt1\\\\Downloads\\\\dataset_BALANCEADO.xlsx\")\n",
    "X = df.drop('CONDUCTA', axis=1)\n",
    "y = df['CONDUCTA']\n",
    "\n",
    "# Definir el espacio de búsqueda de hiperparámetros\n",
    "param_grid = {\n",
    "    'n_estimators': [50],\n",
    "    'criterion': ['entropy'],\n",
    "    'max_depth': [20],\n",
    "    'min_samples_split': [4,],\n",
    "    'min_samples_leaf': [1],\n",
    "    'max_features': [0.25],\n",
    "    'bootstrap': [True],\n",
    "    'learning_rate': [0.1],\n",
    "    'subsample': [0.8, 0.9],\n",
    "    'colsample_bytree': [1.0],\n",
    "    'num_leaves': [31],\n",
    "    'gamma': [1],\n",
    "    'metric':['logloss'], #FALTA\n",
    "    'objective':['binary:logistic'],\n",
    "    'max_leaf_nodes': None,\n",
    "    'min_impurity_decrease': 0.0,\n",
    "    'min_weight_fraction_leaf': 0.0,\n",
    "    'min_child_samples': 20,\n",
    "    'feature_fraction': 0.6,\n",
    "}\n",
    "# Crear instancia del clasificador\n",
    "rf_wrapper = RandomForestClassifierWrapper(X, y, n_estimators=100, criterion='gini', max_depth=10, min_samples_split=2,\n",
    "                                           min_samples_leaf=1, max_features=None, bootstrap=True, learning_rate=0.1,\n",
    "                                           subsample=0.8, colsample_bytree=0.8, num_leaves=31, gamma=0)\n",
    "\n",
    "# Entrenar el modelo\n",
    "rf_wrapper.train()\n",
    "\n",
    "# Crear instancia de GridSearchCV\n",
    "grid_search = GridSearchCV(rf_wrapper.rf, param_grid=param_grid, cv=5, scoring='precision_macro', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Entrenar el modelo\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"Mejores hiperparámetros:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"Mejor score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc417da-5880-464b-abb7-5a4c89481a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
