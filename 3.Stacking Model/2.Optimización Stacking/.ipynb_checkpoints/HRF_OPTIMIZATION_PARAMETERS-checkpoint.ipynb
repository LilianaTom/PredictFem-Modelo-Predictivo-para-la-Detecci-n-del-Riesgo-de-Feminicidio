{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "180e3510-e4c9-4e8f-ac96-cc2848b468a4",
   "metadata": {},
   "source": [
    "# Nuestro modelo con búsqueda exhaustiva para selección de parámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5f650-f32b-40e6-b4e0-f55728957ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c23ed4a-2287-4e47-86c8-e149126e9ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cd775e5-42ba-4296-aa96-df8b4b2d0772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esta ejecutando\n",
      "Esta ejecutando\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 906, number of negative: 1115\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000380 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 559\n",
      "[LightGBM] [Info] Number of data points in the train set: 2021, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448293 -> initscore=-0.207570\n",
      "[LightGBM] [Info] Start training from score -0.207570\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 724, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000321 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 554\n",
      "[LightGBM] [Info] Number of data points in the train set: 1616, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448020 -> initscore=-0.208675\n",
      "[LightGBM] [Info] Start training from score -0.208675\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000295 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 551\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000302 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 558\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000257 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 552\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [20:50:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"feature_fraction\", \"metric\", \"min_child_samples\", \"min_impurity_decrease\", \"min_weight_fraction_leaf\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [20:50:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"feature_fraction\", \"metric\", \"min_child_samples\", \"min_impurity_decrease\", \"min_weight_fraction_leaf\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000421 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 553\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [20:51:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"feature_fraction\", \"metric\", \"min_child_samples\", \"min_impurity_decrease\", \"min_weight_fraction_leaf\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [20:51:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"feature_fraction\", \"metric\", \"min_child_samples\", \"min_impurity_decrease\", \"min_weight_fraction_leaf\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 906, number of negative: 1115\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000620 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 559\n",
      "[LightGBM] [Info] Number of data points in the train set: 2021, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448293 -> initscore=-0.207570\n",
      "[LightGBM] [Info] Start training from score -0.207570\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 724, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000392 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 554\n",
      "[LightGBM] [Info] Number of data points in the train set: 1616, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448020 -> initscore=-0.208675\n",
      "[LightGBM] [Info] Start training from score -0.208675\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000325 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 553\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000325 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 558\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000310 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 552\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000323 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 553\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "Mejores hiperparámetros:\n",
      "{'bootstrap': True, 'colsample_bytree': 0.8, 'criterion': 'entropy', 'feature_fraction': 0.6, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 20, 'max_features': 'sqrt', 'metric': 'logloss', 'min_child_samples': 10, 'min_impurity_decrease': 0.1, 'min_samples_leaf': 1, 'min_samples_split': 4, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 50, 'num_leaves': 40, 'objective': 'binary:logistic', 'subsample': 0.8}\n",
      "Mejor score:  0.9392789035120028\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "print(\"Esta ejecutando\")\n",
    "class RandomForest:\n",
    "    def __init__(self, n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, \n",
    "                 max_features, bootstrap, learning_rate, subsample, colsample_bytree, num_leaves, gamma,\n",
    "                 objective='binary:logistic', max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                 min_weight_fraction_leaf=0.0, min_child_samples=20, feature_fraction=0.6,  metric='logloss'):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.bootstrap = bootstrap\n",
    "        self.random_state = 42\n",
    "        self.stacking_model = None  \n",
    "        self.predictions = []\n",
    "        self.learning_rate = learning_rate  \n",
    "        self.subsample = subsample  \n",
    "        self.colsample_bytree = colsample_bytree  \n",
    "        self.num_leaves = num_leaves\n",
    "        self.gamma = gamma\n",
    "        self.objective = objective\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.min_child_samples = min_child_samples\n",
    "        self.feature_fraction = feature_fraction\n",
    "        self.metric = metric\n",
    "\n",
    "    def fit(self, X, y):        \n",
    "        n_estimators_per_model = self.n_estimators // 3\n",
    "        # Definir el clasificador de nivel 1\n",
    "        estimators = [\n",
    "            ('xgb', XGBClassifier(n_estimators=n_estimators_per_model, max_depth=self.max_depth, random_state=self.random_state,\n",
    "                                  learning_rate=self.learning_rate, subsample=self.subsample, colsample_bytree=self.colsample_bytree,\n",
    "                                  gamma=self.gamma, objective=self.objective, max_leaf_nodes=self.max_leaf_nodes,\n",
    "                                  min_impurity_decrease=self.min_impurity_decrease, min_weight_fraction_leaf=self.min_weight_fraction_leaf,\n",
    "                                  min_child_samples=self.min_child_samples, feature_fraction=self.feature_fraction,metric=self.metric)),\n",
    "            ('lgbm', LGBMClassifier(n_estimators=n_estimators_per_model, max_depth=self.max_depth, random_state=self.random_state,\n",
    "                                    learning_rate=self.learning_rate, subsample=self.subsample, colsample_bytree=self.colsample_bytree,\n",
    "                                    num_leaves=self.num_leaves, max_leaf_nodes=self.max_leaf_nodes,\n",
    "                                    min_child_samples=self.min_child_samples, feature_fraction=self.feature_fraction)),\n",
    "            ('extra_trees', ExtraTreesClassifier(n_estimators=n_estimators_per_model, criterion=self.criterion,\n",
    "                                                 max_depth=self.max_depth, min_samples_split=self.min_samples_split,\n",
    "                                                 min_samples_leaf=self.min_samples_leaf, max_features=self.max_features,\n",
    "                                                 bootstrap=self.bootstrap, random_state=self.random_state))]\n",
    "            \n",
    "        # Definir el clasificador de nivel 2 (modelo base)\n",
    "        rf_base = RandomForestClassifier(n_estimators=self.n_estimators, random_state=self.random_state)\n",
    "        \n",
    "        # Crear el modelo de stacking\n",
    "        self.stacking_model = StackingClassifier(estimators=estimators, final_estimator=rf_base)\n",
    "        self.stacking_model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.stacking_model.predict(X)\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'criterion': self.criterion,\n",
    "            'max_depth': self.max_depth,\n",
    "            'min_samples_split': self.min_samples_split,\n",
    "            'min_samples_leaf': self.min_samples_leaf,\n",
    "            'max_features': self.max_features,\n",
    "            'bootstrap': self.bootstrap,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'subsample': self.subsample,\n",
    "            'colsample_bytree': self.colsample_bytree,\n",
    "            'num_leaves': self.num_leaves,\n",
    "            'gamma': self.gamma,\n",
    "            'objective': self.objective,\n",
    "            'max_leaf_nodes': self.max_leaf_nodes,\n",
    "            'min_impurity_decrease': self.min_impurity_decrease,\n",
    "            'min_weight_fraction_leaf': self.min_weight_fraction_leaf,\n",
    "            'min_child_samples': self.min_child_samples,\n",
    "            'feature_fraction': self.feature_fraction,\n",
    "            'metric': self.metric\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n",
    "\n",
    "class RandomForestClassifierWrapper:\n",
    "    def __init__(self, X, y, n_estimators, criterion, max_depth, min_samples_split,\n",
    "                 min_samples_leaf, max_features, bootstrap, learning_rate, subsample, colsample_bytree, num_leaves, gamma,\n",
    "                 objective='binary', max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                 min_weight_fraction_leaf=0.0, min_child_samples=20, feature_fraction=0.6):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        self.X_train = self.X_train.values\n",
    "        self.y_train = self.y_train.values\n",
    "        \n",
    "        self.rf = RandomForest(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth,\n",
    "                                min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
    "                                max_features=max_features, bootstrap=bootstrap,\n",
    "                                learning_rate=learning_rate, subsample=subsample,\n",
    "                                colsample_bytree=colsample_bytree, num_leaves=num_leaves, gamma=gamma,\n",
    "                                objective=objective, max_leaf_nodes=max_leaf_nodes,\n",
    "                                min_impurity_decrease=min_impurity_decrease, min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "                                min_child_samples=min_child_samples, feature_fraction=feature_fraction)\n",
    "        self.estimators = []\n",
    "\n",
    "    def train(self):\n",
    "        self.rf.fit(self.X_train, self.y_train)\n",
    "        \n",
    "    def predict(self):\n",
    "        return self.rf.predict(self.X_test)\n",
    "\n",
    "    def evaluate(self):\n",
    "        y_pred = self.predict()        \n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        precision = precision_score(self.y_test, y_pred, average='macro')\n",
    "        recall = recall_score(self.y_test, y_pred, average='macro')\n",
    "        f1 = f1_score(self.y_test, y_pred, average='macro')\n",
    "        return accuracy, precision, recall, f1\n",
    "\n",
    "import os\n",
    "# Obtener la ruta del directorio actual\n",
    "ruta_actual = os.getcwd()\n",
    "# Concatenar el nombre del archivo al final de la ruta actual\n",
    "archivo = os.path.join(ruta_actual, 'Balanced_Data_Set.xlsx')\n",
    "# Cargar datos\n",
    "df = pd.read_excel(archivo)\n",
    "# Leer datos\n",
    "#df = pd.read_excel(\"C:\\\\Users\\\\klgt1\\\\Downloads\\\\dataset_BALANCEADO.xlsx\")\n",
    "X = df.drop('CONDUCTA', axis=1)\n",
    "y = df['CONDUCTA']\n",
    "\n",
    "\n",
    "print(\"Esta ejecutando\")\n",
    "# Definir el espacio de búsqueda de hiperparámetros\n",
    "param_grid = {\n",
    "    'n_estimators': [20,50],\n",
    "    'criterion': ['entropy'],\n",
    "    'max_depth': [5,20],\n",
    "    'min_samples_split': [2,4],\n",
    "    'min_samples_leaf': [1],\n",
    "    'max_features': ['sqrt','log2'],\n",
    "    'bootstrap': [True],\n",
    "    'learning_rate': [0.001,0.1],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8],\n",
    "    'num_leaves': [40],\n",
    "    'gamma': [0],\n",
    "    'metric':['logloss'], #FALTA\n",
    "    'objective':['binary:logistic'],\n",
    "    'min_impurity_decrease': [0.1],\n",
    "    'min_weight_fraction_leaf': [0.0],\n",
    "    'min_child_samples': [10],\n",
    "    'feature_fraction': [0.6],\n",
    "}\n",
    "# Crear instancia del clasificador\n",
    "rf_wrapper = RandomForestClassifierWrapper(X, y, n_estimators=50, criterion='entropy', max_depth=20, min_samples_split=4,\n",
    "                                           min_samples_leaf=1, max_features=0.25, bootstrap=True, learning_rate=0.1,\n",
    "                                           subsample=0.8, colsample_bytree=1.0, num_leaves=31, gamma=1,\n",
    "                                           objective='binary:logistic')\n",
    "\n",
    "\n",
    "# Entrenar el modelo\n",
    "rf_wrapper.train()\n",
    "\n",
    "# Crear instancia de GridSearchCV\n",
    "grid_search = GridSearchCV(rf_wrapper.rf, param_grid=param_grid, cv=5, scoring='precision', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Entrenar el modelo\n",
    "grid_search.fit(rf_wrapper.X_train, rf_wrapper.y_train)\n",
    "\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"Mejores hiperparámetros:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"Mejor score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "792ec874-8829-44b7-936b-939156acf764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esta ejecutando\n",
      "Esta ejecutando\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 906, number of negative: 1115\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000205 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 559\n",
      "[LightGBM] [Info] Number of data points in the train set: 2021, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448293 -> initscore=-0.207570\n",
      "[LightGBM] [Info] Start training from score -0.207570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [12:23:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"feature_fraction\", \"metric\", \"min_child_samples\", \"min_impurity_decrease\", \"min_weight_fraction_leaf\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n",
      "C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [12:23:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"feature_fraction\", \"metric\", \"min_child_samples\", \"min_impurity_decrease\", \"min_weight_fraction_leaf\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 724, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000354 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 554\n",
      "[LightGBM] [Info] Number of data points in the train set: 1616, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448020 -> initscore=-0.208675\n",
      "[LightGBM] [Info] Start training from score -0.208675\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000286 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 551\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000286 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 558\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000267 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 552\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000255 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 553\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [12:24:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"feature_fraction\", \"metric\", \"min_child_samples\", \"min_impurity_decrease\", \"min_weight_fraction_leaf\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 906, number of negative: 1115\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000413 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 559\n",
      "[LightGBM] [Info] Number of data points in the train set: 2021, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448293 -> initscore=-0.207570\n",
      "[LightGBM] [Info] Start training from score -0.207570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [12:24:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"feature_fraction\", \"metric\", \"min_child_samples\", \"min_impurity_decrease\", \"min_weight_fraction_leaf\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [12:24:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"feature_fraction\", \"metric\", \"min_child_samples\", \"min_impurity_decrease\", \"min_weight_fraction_leaf\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 724, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000235 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 554\n",
      "[LightGBM] [Info] Number of data points in the train set: 1616, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448020 -> initscore=-0.208675\n",
      "[LightGBM] [Info] Start training from score -0.208675\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000422 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 553\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000389 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 558\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000332 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 552\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000354 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 553\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "Mejores hiperparámetros:\n",
      "{'bootstrap': True, 'colsample_bytree': 0.8, 'criterion': 'entropy', 'feature_fraction': 0.6, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 20, 'max_features': 'sqrt', 'metric': 'logloss', 'min_child_samples': 10, 'min_impurity_decrease': 0.1, 'min_samples_leaf': 1, 'min_samples_split': 4, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 50, 'num_leaves': 40, 'objective': 'binary:logistic', 'subsample': 0.8}\n",
      "Mejor score:  0.9564382791015749\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "print(\"Esta ejecutando\")\n",
    "class RandomForest:\n",
    "    def __init__(self, n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, \n",
    "                 max_features, bootstrap, learning_rate, subsample, colsample_bytree, num_leaves, gamma,\n",
    "                 objective='binary:logistic', max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                 min_weight_fraction_leaf=0.0, min_child_samples=20, feature_fraction=0.6,  metric='logloss'):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.bootstrap = bootstrap\n",
    "        self.random_state = 42\n",
    "        self.stacking_model = None  \n",
    "        self.predictions = []\n",
    "        self.learning_rate = learning_rate  \n",
    "        self.subsample = subsample  \n",
    "        self.colsample_bytree = colsample_bytree  \n",
    "        self.num_leaves = num_leaves\n",
    "        self.gamma = gamma\n",
    "        self.objective = objective\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.min_child_samples = min_child_samples\n",
    "        self.feature_fraction = feature_fraction\n",
    "        self.metric = metric\n",
    "\n",
    "    def fit(self, X, y):        \n",
    "        # Definir el clasificador de nivel 1\n",
    "        estimators = [\n",
    "            ('xgb', XGBClassifier(n_estimators=self.n_estimators, max_depth=self.max_depth, random_state=self.random_state,\n",
    "                                  learning_rate=self.learning_rate, subsample=self.subsample, colsample_bytree=self.colsample_bytree,\n",
    "                                  gamma=self.gamma, objective=self.objective, max_leaf_nodes=self.max_leaf_nodes,\n",
    "                                  min_impurity_decrease=self.min_impurity_decrease, min_weight_fraction_leaf=self.min_weight_fraction_leaf,\n",
    "                                  min_child_samples=self.min_child_samples, feature_fraction=self.feature_fraction,metric=self.metric)),\n",
    "            ('lgbm', LGBMClassifier(n_estimators=self.n_estimators, max_depth=self.max_depth, random_state=self.random_state,\n",
    "                                    learning_rate=self.learning_rate, subsample=self.subsample, colsample_bytree=self.colsample_bytree,\n",
    "                                    num_leaves=self.num_leaves, max_leaf_nodes=self.max_leaf_nodes,\n",
    "                                    min_child_samples=self.min_child_samples, feature_fraction=self.feature_fraction)),\n",
    "            ('extra_trees', ExtraTreesClassifier(n_estimators=self.n_estimators, criterion=self.criterion,\n",
    "                                                 max_depth=self.max_depth, min_samples_split=self.min_samples_split,\n",
    "                                                 min_samples_leaf=self.min_samples_leaf, max_features=self.max_features,\n",
    "                                                 bootstrap=self.bootstrap, random_state=self.random_state))]\n",
    "            \n",
    "        # Definir el clasificador de nivel 2 (modelo base)\n",
    "        rf_base = RandomForestClassifier(n_estimators=self.n_estimators, random_state=self.random_state)\n",
    "        \n",
    "        # Crear el modelo de stacking\n",
    "        self.stacking_model = StackingClassifier(estimators=estimators, final_estimator=rf_base)\n",
    "        self.stacking_model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.stacking_model.predict(X)\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'criterion': self.criterion,\n",
    "            'max_depth': self.max_depth,\n",
    "            'min_samples_split': self.min_samples_split,\n",
    "            'min_samples_leaf': self.min_samples_leaf,\n",
    "            'max_features': self.max_features,\n",
    "            'bootstrap': self.bootstrap,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'subsample': self.subsample,\n",
    "            'colsample_bytree': self.colsample_bytree,\n",
    "            'num_leaves': self.num_leaves,\n",
    "            'gamma': self.gamma,\n",
    "            'objective': self.objective,\n",
    "            'max_leaf_nodes': self.max_leaf_nodes,\n",
    "            'min_impurity_decrease': self.min_impurity_decrease,\n",
    "            'min_weight_fraction_leaf': self.min_weight_fraction_leaf,\n",
    "            'min_child_samples': self.min_child_samples,\n",
    "            'feature_fraction': self.feature_fraction,\n",
    "            'metric': self.metric\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n",
    "\n",
    "class RandomForestClassifierWrapper:\n",
    "    def __init__(self, X, y, n_estimators, criterion, max_depth, min_samples_split,\n",
    "                 min_samples_leaf, max_features, bootstrap, learning_rate, subsample, colsample_bytree, num_leaves, gamma,\n",
    "                 objective='binary', max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                 min_weight_fraction_leaf=0.0, min_child_samples=20, feature_fraction=0.6):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        self.X_train = self.X_train.values\n",
    "        self.y_train = self.y_train.values\n",
    "        \n",
    "        self.rf = RandomForest(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth,\n",
    "                                min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
    "                                max_features=max_features, bootstrap=bootstrap,\n",
    "                                learning_rate=learning_rate, subsample=subsample,\n",
    "                                colsample_bytree=colsample_bytree, num_leaves=num_leaves, gamma=gamma,\n",
    "                                objective=objective, max_leaf_nodes=max_leaf_nodes,\n",
    "                                min_impurity_decrease=min_impurity_decrease, min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "                                min_child_samples=min_child_samples, feature_fraction=feature_fraction)\n",
    "        self.estimators = []\n",
    "\n",
    "    def train(self):\n",
    "        self.rf.fit(self.X_train, self.y_train)\n",
    "        \n",
    "    def predict(self):\n",
    "        return self.rf.predict(self.X_test)\n",
    "\n",
    "    def evaluate(self):\n",
    "        y_pred = self.predict()        \n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        precision = precision_score(self.y_test, y_pred, average='macro')\n",
    "        recall = recall_score(self.y_test, y_pred, average='macro')\n",
    "        f1 = f1_score(self.y_test, y_pred, average='macro')\n",
    "        return accuracy, precision, recall, f1\n",
    "\n",
    "import os\n",
    "# Obtener la ruta del directorio actual\n",
    "ruta_actual = os.getcwd()\n",
    "# Concatenar el nombre del archivo al final de la ruta actual\n",
    "archivo = os.path.join(ruta_actual, 'Balanced_Data_Set.xlsx')\n",
    "# Cargar datos\n",
    "df = pd.read_excel(archivo)\n",
    "# Leer datos\n",
    "#df = pd.read_excel(\"C:\\\\Users\\\\klgt1\\\\Downloads\\\\dataset_BALANCEADO.xlsx\")\n",
    "X = df.drop('CONDUCTA', axis=1)\n",
    "y = df['CONDUCTA']\n",
    "\n",
    "\n",
    "print(\"Esta ejecutando\")\n",
    "# Definir el espacio de búsqueda de hiperparámetros\n",
    "param_grid = {\n",
    "    'n_estimators': [20,50],\n",
    "    'criterion': ['entropy'],\n",
    "    'max_depth': [5,20],\n",
    "    'min_samples_split': [2,4],\n",
    "    'min_samples_leaf': [1],\n",
    "    'max_features': ['sqrt','log2'],\n",
    "    'bootstrap': [True],\n",
    "    'learning_rate': [0.001,0.1],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8],\n",
    "    'num_leaves': [40],\n",
    "    'gamma': [0],\n",
    "    'metric':['logloss'], #FALTA\n",
    "    'objective':['binary:logistic'],\n",
    "    'min_impurity_decrease': [0.1],\n",
    "    'min_weight_fraction_leaf': [0.0],\n",
    "    'min_child_samples': [10],\n",
    "    'feature_fraction': [0.6],\n",
    "}\n",
    "# Crear instancia del clasificador\n",
    "rf_wrapper = RandomForestClassifierWrapper(X, y, n_estimators=50, criterion='entropy', max_depth=20, min_samples_split=4,\n",
    "                                           min_samples_leaf=1, max_features=0.25, bootstrap=True, learning_rate=0.1,\n",
    "                                           subsample=0.8, colsample_bytree=1.0, num_leaves=31, gamma=1,\n",
    "                                           objective='binary:logistic')\n",
    "\n",
    "\n",
    "# Entrenar el modelo\n",
    "rf_wrapper.train()\n",
    "\n",
    "# Crear instancia de GridSearchCV\n",
    "grid_search = GridSearchCV(rf_wrapper.rf, param_grid=param_grid, cv=5, scoring='precision', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Entrenar el modelo\n",
    "grid_search.fit(rf_wrapper.X_train, rf_wrapper.y_train)\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"Mejores hiperparámetros:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"Mejor score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11357210-40ad-4d38-9013-4d2d65a9407c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esta ejecutando\n",
      "Esta ejecutando\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 906, number of negative: 1115\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 559\n",
      "[LightGBM] [Info] Number of data points in the train set: 2021, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448293 -> initscore=-0.207570\n",
      "[LightGBM] [Info] Start training from score -0.207570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [00:30:23] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"feature_fraction\", \"metric\", \"min_child_samples\", \"min_impurity_decrease\", \"min_weight_fraction_leaf\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [00:30:23] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"feature_fraction\", \"metric\", \"min_child_samples\", \"min_impurity_decrease\", \"min_weight_fraction_leaf\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 724, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000304 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 554\n",
      "[LightGBM] [Info] Number of data points in the train set: 1616, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448020 -> initscore=-0.208675\n",
      "[LightGBM] [Info] Start training from score -0.208675\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000256 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 551\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 31\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000321 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 558\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000311 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 552\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000299 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 553\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [00:31:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"feature_fraction\", \"metric\", \"min_child_samples\", \"min_impurity_decrease\", \"min_weight_fraction_leaf\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 906, number of negative: 1115\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000438 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 559\n",
      "[LightGBM] [Info] Number of data points in the train set: 2021, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448293 -> initscore=-0.207570\n",
      "[LightGBM] [Info] Start training from score -0.207570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [00:31:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"feature_fraction\", \"metric\", \"min_child_samples\", \"min_impurity_decrease\", \"min_weight_fraction_leaf\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 724, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000486 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 554\n",
      "[LightGBM] [Info] Number of data points in the train set: 1616, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448020 -> initscore=-0.208675\n",
      "[LightGBM] [Info] Start training from score -0.208675\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000396 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 553\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000392 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 558\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000397 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 552\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000317 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 553\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "Mejores hiperparámetros:\n",
      "{'bootstrap': True, 'colsample_bytree': 0.8, 'criterion': 'entropy', 'feature_fraction': 0.6, 'gamma': 0, 'learning_rate': 0.1, 'max_depth': 20, 'max_features': 'sqrt', 'metric': 'logloss', 'min_child_samples': 10, 'min_impurity_decrease': 0.1, 'min_samples_leaf': 1, 'min_samples_split': 4, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 50, 'num_leaves': 40, 'objective': 'binary:logistic', 'subsample': 0.8}\n",
      "Mejor score:  0.9525516205549962\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but ExtraTreesClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precisión del modelo: 94.07%\n",
      "Exactitud del modelo: 93.87%\n",
      "Recall del modelo: 93.43%\n",
      "F1-score del modelo: 93.70%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "print(\"Esta ejecutando\")\n",
    "class RandomForest:\n",
    "    def __init__(self, n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, \n",
    "                 max_features, bootstrap, learning_rate, subsample, colsample_bytree, num_leaves, gamma,\n",
    "                 objective='binary:logistic', max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                 min_weight_fraction_leaf=0.0, min_child_samples=20, feature_fraction=0.6,  metric='logloss'):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.bootstrap = bootstrap\n",
    "        self.random_state = 42\n",
    "        self.stacking_model = None  \n",
    "        self.predictions = []\n",
    "        self.learning_rate = learning_rate  \n",
    "        self.subsample = subsample  \n",
    "        self.colsample_bytree = colsample_bytree  \n",
    "        self.num_leaves = num_leaves\n",
    "        self.gamma = gamma\n",
    "        self.objective = objective\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.min_child_samples = min_child_samples\n",
    "        self.feature_fraction = feature_fraction\n",
    "        self.metric = metric\n",
    "\n",
    "    def fit(self, X, y):        \n",
    "        # Definir el clasificador de nivel 1\n",
    "        estimators = [\n",
    "            ('xgb', XGBClassifier(n_estimators=self.n_estimators, max_depth=self.max_depth, random_state=self.random_state,\n",
    "                                  learning_rate=self.learning_rate, subsample=self.subsample, colsample_bytree=self.colsample_bytree,\n",
    "                                  gamma=self.gamma, objective=self.objective, max_leaf_nodes=self.max_leaf_nodes,\n",
    "                                  min_impurity_decrease=self.min_impurity_decrease, min_weight_fraction_leaf=self.min_weight_fraction_leaf,\n",
    "                                  min_child_samples=self.min_child_samples, feature_fraction=self.feature_fraction,metric=self.metric)),\n",
    "            ('lgbm', LGBMClassifier(n_estimators=self.n_estimators, max_depth=self.max_depth, random_state=self.random_state,\n",
    "                                    learning_rate=self.learning_rate, subsample=self.subsample, colsample_bytree=self.colsample_bytree,\n",
    "                                    num_leaves=self.num_leaves, max_leaf_nodes=self.max_leaf_nodes,\n",
    "                                    min_child_samples=self.min_child_samples, feature_fraction=self.feature_fraction)),\n",
    "            ('extra_trees', ExtraTreesClassifier(n_estimators=self.n_estimators, criterion=self.criterion,\n",
    "                                                 max_depth=self.max_depth, min_samples_split=self.min_samples_split,\n",
    "                                                 min_samples_leaf=self.min_samples_leaf, max_features=self.max_features,\n",
    "                                                 bootstrap=self.bootstrap, random_state=self.random_state))]\n",
    "            \n",
    "        # Definir el clasificador de nivel 2 (modelo base)\n",
    "        rf_base = RandomForestClassifier(n_estimators=self.n_estimators, criterion=self.criterion,max_depth=self.max_depth,\n",
    "                                         min_samples_split=self.min_samples_split,min_samples_leaf=self.min_samples_leaf,\n",
    "                                         max_features=self.max_features,bootstrap=self.bootstrap, random_state=self.random_state)\n",
    "        \n",
    "        # Crear el modelo de stacking\n",
    "        self.stacking_model = StackingClassifier(estimators=estimators, final_estimator=rf_base)\n",
    "        self.stacking_model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.stacking_model.predict(X)\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'criterion': self.criterion,\n",
    "            'max_depth': self.max_depth,\n",
    "            'min_samples_split': self.min_samples_split,\n",
    "            'min_samples_leaf': self.min_samples_leaf,\n",
    "            'max_features': self.max_features,\n",
    "            'bootstrap': self.bootstrap,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'subsample': self.subsample,\n",
    "            'colsample_bytree': self.colsample_bytree,\n",
    "            'num_leaves': self.num_leaves,\n",
    "            'gamma': self.gamma,\n",
    "            'objective': self.objective,\n",
    "            'max_leaf_nodes': self.max_leaf_nodes,\n",
    "            'min_impurity_decrease': self.min_impurity_decrease,\n",
    "            'min_weight_fraction_leaf': self.min_weight_fraction_leaf,\n",
    "            'min_child_samples': self.min_child_samples,\n",
    "            'feature_fraction': self.feature_fraction,\n",
    "            'metric': self.metric\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n",
    "\n",
    "class RandomForestClassifierWrapper:\n",
    "    def __init__(self, X, y, n_estimators, criterion, max_depth, min_samples_split,\n",
    "                 min_samples_leaf, max_features, bootstrap, learning_rate, subsample, colsample_bytree, num_leaves, gamma,\n",
    "                 objective='binary', max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                 min_weight_fraction_leaf=0.0, min_child_samples=20, feature_fraction=0.6):\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        self.X_train = self.X_train.values\n",
    "        self.y_train = self.y_train.values\n",
    "        \n",
    "        self.rf = RandomForest(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth,\n",
    "                                min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
    "                                max_features=max_features, bootstrap=bootstrap,\n",
    "                                learning_rate=learning_rate, subsample=subsample,\n",
    "                                colsample_bytree=colsample_bytree, num_leaves=num_leaves, gamma=gamma,\n",
    "                                objective=objective, max_leaf_nodes=max_leaf_nodes,\n",
    "                                min_impurity_decrease=min_impurity_decrease, min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "                                min_child_samples=min_child_samples, feature_fraction=feature_fraction)\n",
    "        self.estimators = []\n",
    "\n",
    "    def train(self):\n",
    "        self.rf.fit(self.X_train, self.y_train)\n",
    "        \n",
    "    def predict(self):\n",
    "        return self.rf.predict(self.X_test)\n",
    "\n",
    "    def evaluate(self):\n",
    "        y_pred = self.predict()        \n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        precision = precision_score(self.y_test, y_pred, average='macro')\n",
    "        recall = recall_score(self.y_test, y_pred, average='macro')\n",
    "        f1 = f1_score(self.y_test, y_pred, average='macro')\n",
    "        return accuracy, precision, recall, f1\n",
    "\n",
    "import os\n",
    "# Obtener la ruta del directorio actual\n",
    "ruta_actual = os.getcwd()\n",
    "# Concatenar el nombre del archivo al final de la ruta actual\n",
    "archivo = os.path.join(ruta_actual, 'Balanced_Data_Set.xlsx')\n",
    "# Cargar datos\n",
    "df = pd.read_excel(archivo)\n",
    "# Leer datos\n",
    "#df = pd.read_excel(\"C:\\\\Users\\\\klgt1\\\\Downloads\\\\dataset_BALANCEADO.xlsx\")\n",
    "X = df.drop('CONDUCTA', axis=1)\n",
    "y = df['CONDUCTA']\n",
    "\n",
    "\n",
    "print(\"Esta ejecutando\")\n",
    "# Definir el espacio de búsqueda de hiperparámetros\n",
    "param_grid = {\n",
    "    'n_estimators': [20,50],\n",
    "    'criterion': ['entropy'],\n",
    "    'max_depth': [5,20],\n",
    "    'min_samples_split': [2,4],\n",
    "    'min_samples_leaf': [1],\n",
    "    'max_features': ['sqrt','log2'],\n",
    "    'bootstrap': [True],\n",
    "    'learning_rate': [0.001,0.1],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8],\n",
    "    'num_leaves': [40],\n",
    "    'gamma': [0],\n",
    "    'metric':['logloss'], #FALTA\n",
    "    'objective':['binary:logistic'],\n",
    "    'min_impurity_decrease': [0.1],\n",
    "    'min_weight_fraction_leaf': [0.0],\n",
    "    'min_child_samples': [10],\n",
    "    'feature_fraction': [0.6],\n",
    "}\n",
    "# Crear instancia del clasificador\n",
    "rf_wrapper = RandomForestClassifierWrapper(X, y, n_estimators=50, criterion='entropy', max_depth=20, min_samples_split=4,\n",
    "                                           min_samples_leaf=1, max_features=0.25, bootstrap=True, learning_rate=0.1,\n",
    "                                           subsample=0.8, colsample_bytree=1.0, num_leaves=31, gamma=1,\n",
    "                                           objective='binary:logistic')\n",
    "\n",
    "\n",
    "# Entrenar el modelo\n",
    "rf_wrapper.train()\n",
    "\n",
    "# Crear instancia de GridSearchCV\n",
    "grid_search = GridSearchCV(rf_wrapper.rf, param_grid=param_grid, cv=5, scoring='precision', verbose=2, n_jobs=-1)\n",
    "\n",
    "# Entrenar el modelo\n",
    "grid_search.fit(rf_wrapper.X_train, rf_wrapper.y_train)\n",
    "# Imprimir resultados\n",
    "print(\"Mejores hiperparámetros:\")\n",
    "print(grid_search.best_params_)\n",
    "print(\"Mejor score: \", grid_search.best_score_)\n",
    "\n",
    "\n",
    "# Obtener predicciones en el conjunto de prueba\n",
    "y_pred_test = grid_search.predict(rf_wrapper.X_test)\n",
    "\n",
    "# Calcular matriz de confusión\n",
    "conf_matrix = confusion_matrix(rf_wrapper.y_test, y_pred_test)\n",
    "\n",
    "# Visualizar la matriz de confusión\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=['Clase 0', 'Clase 1'], yticklabels=['Clase 0', 'Clase 1'])\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Calcular métricas\n",
    "accuracy = accuracy_score(rf_wrapper.y_test, y_pred_test)\n",
    "precision = precision_score(rf_wrapper.y_test, y_pred_test, average='macro')\n",
    "recall = recall_score(rf_wrapper.y_test, y_pred_test, average='macro')\n",
    "f1 = f1_score(rf_wrapper.y_test, y_pred_test, average='macro')\n",
    "\n",
    "# Mostrar métricas\n",
    "print(f'\\nPrecisión del modelo: {precision * 100:.2f}%')\n",
    "print(f'Exactitud del modelo: {accuracy * 100:.2f}%')\n",
    "print(f'Recall del modelo: {recall * 100:.2f}%')\n",
    "print(f'F1-score del modelo: {f1 * 100:.2f}%\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
