{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1210b47a-8f51-44eb-a8ac-0b6c8c50a36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "print(\"Esta ejecutando\")\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, \n",
    "                 max_features, bootstrap, learning_rate, subsample, colsample_bytree, num_leaves, gamma,\n",
    "                 objective='binary:logistic', max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                 min_weight_fraction_leaf=0.0, min_child_samples=20, feature_fraction=0.6,  metric='logloss'):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.bootstrap = bootstrap\n",
    "        self.random_state = 42\n",
    "        self.stacking_model = None  \n",
    "        self.predictions = []\n",
    "        self.learning_rate = learning_rate  \n",
    "        self.subsample = subsample  \n",
    "        self.colsample_bytree = colsample_bytree  \n",
    "        self.num_leaves = num_leaves\n",
    "        self.gamma = gamma\n",
    "        self.objective = objective\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.min_child_samples = min_child_samples\n",
    "        self.feature_fraction = feature_fraction\n",
    "        self.metric = metric\n",
    "\n",
    "    def fit(self, X, y):        \n",
    "        # Definir el clasificador de nivel 1\n",
    "        estimators = [\n",
    "            ('xgb', XGBClassifier(n_estimators=self.n_estimators, max_depth=self.max_depth, random_state=self.random_state,\n",
    "                                  learning_rate=self.learning_rate, subsample=self.subsample, colsample_bytree=self.colsample_bytree,\n",
    "                                  gamma=self.gamma, objective=self.objective, max_leaf_nodes=self.max_leaf_nodes,\n",
    "                                  min_impurity_decrease=self.min_impurity_decrease, min_weight_fraction_leaf=self.min_weight_fraction_leaf,\n",
    "                                  min_child_samples=self.min_child_samples, feature_fraction=self.feature_fraction,metric=self.metric)),\n",
    "            \n",
    "            ('lgbm', LGBMClassifier(n_estimators=self.n_estimators, max_depth=self.max_depth, random_state=self.random_state,\n",
    "                                    learning_rate=self.learning_rate, subsample=self.subsample, colsample_bytree=self.colsample_bytree,\n",
    "                                    num_leaves=self.num_leaves, max_leaf_nodes=self.max_leaf_nodes,\n",
    "                                    min_child_samples=self.min_child_samples, feature_fraction=self.feature_fraction)),\n",
    "            \n",
    "            ('extra_trees', ExtraTreesClassifier(n_estimators=self.n_estimators, criterion=self.criterion,\n",
    "                                                 max_depth=self.max_depth, min_samples_split=self.min_samples_split,\n",
    "                                                 min_samples_leaf=self.min_samples_leaf, max_features=self.max_features,\n",
    "                                                 bootstrap=self.bootstrap, random_state=self.random_state))]\n",
    "            \n",
    "        # Definir el clasificador de nivel 2 (modelo base)\n",
    "        rf_base = RandomForestClassifier(n_estimators=self.n_estimators, random_state=self.random_state)\n",
    "        \n",
    "        # Crear el modelo de stacking\n",
    "        self.stacking_model = StackingClassifier(estimators=estimators, final_estimator=rf_base)\n",
    "        self.stacking_model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.stacking_model.predict(X)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'criterion': self.criterion,\n",
    "            'max_depth': self.max_depth,\n",
    "            'min_samples_split': self.min_samples_split,\n",
    "            'min_samples_leaf': self.min_samples_leaf,\n",
    "            'max_features': self.max_features,\n",
    "            'bootstrap': self.bootstrap,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'subsample': self.subsample,\n",
    "            'colsample_bytree': self.colsample_bytree,\n",
    "            'num_leaves': self.num_leaves,\n",
    "            'gamma': self.gamma,\n",
    "            'objective': self.objective,\n",
    "            'max_leaf_nodes': self.max_leaf_nodes,\n",
    "            'min_impurity_decrease': self.min_impurity_decrease,\n",
    "            'min_weight_fraction_leaf': self.min_weight_fraction_leaf,\n",
    "            'min_child_samples': self.min_child_samples,\n",
    "            'feature_fraction': self.feature_fraction,\n",
    "            'metric': self.metric\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n",
    "\n",
    "\n",
    "class RandomForestClassifierWrapper:\n",
    "    def __init__(self, X, y, n_estimators, criterion, max_depth, min_samples_split,\n",
    "                 min_samples_leaf, max_features, bootstrap, learning_rate, subsample, colsample_bytree, num_leaves, gamma,\n",
    "                 objective='binary:logistic', max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                 min_weight_fraction_leaf=0.0, min_child_samples=20, feature_fraction=0.6, metric='logloss'):\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        self.X_train = self.X_train.values\n",
    "        self.y_train = self.y_train.values\n",
    "        \n",
    "        self.rf = RandomForest(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth,\n",
    "                                min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
    "                                max_features=max_features, bootstrap=bootstrap,\n",
    "                                learning_rate=learning_rate, subsample=subsample,\n",
    "                                colsample_bytree=colsample_bytree, num_leaves=num_leaves, gamma=gamma,\n",
    "                                objective=objective, max_leaf_nodes=max_leaf_nodes,\n",
    "                                min_impurity_decrease=min_impurity_decrease, min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "                                min_child_samples=min_child_samples, feature_fraction=feature_fraction,metric = metric)\n",
    "\n",
    "    def train(self):\n",
    "        self.rf.fit(self.X_train, self.y_train)\n",
    "        \n",
    "    def predict(self):\n",
    "        return self.rf.predict(self.X_test)\n",
    "\n",
    "    def evaluate(self):\n",
    "        y_pred = self.predict()        \n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        precision = precision_score(self.y_test, y_pred, average='macro')\n",
    "        recall = recall_score(self.y_test, y_pred, average='macro')\n",
    "        f1 = f1_score(self.y_test, y_pred, average='macro')\n",
    "        return accuracy, precision, recall, f1\n",
    "\n",
    "    def paint_confusion_matrix(self):\n",
    "        # Calcular la matriz de confusión\n",
    "        conf_matrix = confusion_matrix(self.y_test, y_pred)\n",
    "        \n",
    "        # Visualizar la matriz de confusión\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "        plt.xlabel('Predicted labels')\n",
    "        plt.ylabel('True labels')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class Resultado:\n",
    "    def __init__(self, parametros):\n",
    "        self.parametros = parametros\n",
    "        self.metricas = {'Accuracy': None, 'Precision': None, 'Recall': None, 'F1-score': None}\n",
    "\n",
    "    def update_metrics(self, accuracy, precision, recall, f1_score):\n",
    "        self.metricas['Accuracy'] = accuracy\n",
    "        self.metricas['Precision'] = precision\n",
    "        self.metricas['Recall'] = recall\n",
    "        self.metricas['F1-score'] = f1_score\n",
    "        \n",
    "    \n",
    "\n",
    "def loadBalancedDataSet(file_path):\n",
    "    \"\"\"\n",
    "    Carga un conjunto de datos balanceado desde un archivo Excel.   \n",
    "    Returns:\n",
    "    - X (DataFrame): Características del conjunto de datos.\n",
    "    - y (Series): Etiquetas del conjunto de datos.\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(file_path)\n",
    "    X = df.drop('CONDUCTA', axis=1)\n",
    "    y = df['CONDUCTA']\n",
    "    return X, y\n",
    "\n",
    "def loadOptimizedParameters(file_path):\n",
    "    \"\"\"\n",
    "    Carga los parámetros optimizados desde un archivo Excel.\n",
    "    Returns:\n",
    "    - parametros (dict): Parámetros optimizados.\n",
    "    \"\"\"\n",
    "    parametros_df = pd.read_excel(file_path)\n",
    "    fila = parametros_df.iloc[0]\n",
    "    parametros = dict(zip(parametros_df.columns, fila))\n",
    "    return parametros\n",
    "\n",
    "\n",
    "\n",
    "balanced_data_file = \"C:\\\\Users\\\\klgt1\\\\Downloads\\\\dataset_BALANCEADO.xlsx\"\n",
    "optimized_params_file = \"C:\\\\Users\\\\klgt1\\\\Downloads\\\\ParametrosOptimización.xlsx\"\n",
    "\n",
    "X, y = loadBalancedDataSet(balanced_data_file)\n",
    "print(\"Datos balanceados cargados:\")\n",
    "print(X.head())\n",
    "\n",
    "parametros = loadOptimizedParameters(optimized_params_file)\n",
    "print(\"\\nParámetros optimizados cargados:\")\n",
    "print(parametros)\n",
    "\n",
    "# Ejecutar el clasificador con los parámetros actuales\n",
    "wrapper = RandomForestClassifierWrapper(X, y, **parametros)\n",
    "wrapper.train()\n",
    "\n",
    "# Obtener los resultados de print_metrics\n",
    "accuracy, precision, recall, f1_score = wrapper.evaluate()\n",
    "wrraper.paint_confusion_matrix()\n",
    "# Crear un objeto Resultado y actualizar sus métricas\n",
    "resultado = Resultado(parametros)\n",
    "resultado.update_metrics(accuracy, precision, recall, f1_score)\n",
    "\n",
    "print(\"Métricas:\\n\")\n",
    "for metrica, valor in resultado.metricas.items():\n",
    "    print(f\"{metrica}: {valor * 100:.2f}%\")\n",
    "print(\"\\n\\n\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
