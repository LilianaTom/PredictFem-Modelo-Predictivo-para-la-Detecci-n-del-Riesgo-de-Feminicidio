{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1210b47a-8f51-44eb-a8ac-0b6c8c50a36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Esta ejecutando\n",
      "Datos balanceados cargados:\n",
      "   ZONA     SITIO  EDAD  ESCOLARIDAD   Año  ESTADO_CIVIL_CASADO  \\\n",
      "0     0  0.141911  23.0            2  2022                    0   \n",
      "1     0  0.141911  24.0            2  2022                    0   \n",
      "2     0  0.225842  39.0            2  2022                    0   \n",
      "3     0  0.141911  33.0            2  2022                    0   \n",
      "4     0  0.141911  15.0            1  2022                    0   \n",
      "\n",
      "   ESTADO_CIVIL_DIVORCIADO  ESTADO_CIVIL_SOLTERO  ESTADO_CIVIL_UNION_LIBRE  \\\n",
      "0                        0                     1                         0   \n",
      "1                        0                     1                         0   \n",
      "2                        0                     0                         1   \n",
      "3                        0                     1                         0   \n",
      "4                        0                     1                         0   \n",
      "\n",
      "   ESTADO_CIVIL_VIUDO  ...  CARGO_CON_INGRESOS  CARGO_OTROS  \\\n",
      "0                   0  ...                   1            0   \n",
      "1                   0  ...                   1            0   \n",
      "2                   0  ...                   1            0   \n",
      "3                   0  ...                   1            0   \n",
      "4                   0  ...                   0            0   \n",
      "\n",
      "   CARGO_SIN_INGRESOS  MODALIDAD_OTROS  MODALIDAD_VIOLENCIA_FÍSICA  \\\n",
      "0                   0                0                           1   \n",
      "1                   0                0                           1   \n",
      "2                   0                0                           0   \n",
      "3                   0                0                           1   \n",
      "4                   1                1                           0   \n",
      "\n",
      "   MODALIDAD_VIOLENCIA_INTRAFAMILIAR  MODALIDAD_VIOLENCIA_PSICOLOGICA  \\\n",
      "0                                  0                                0   \n",
      "1                                  0                                0   \n",
      "2                                  1                                0   \n",
      "3                                  0                                0   \n",
      "4                                  0                                0   \n",
      "\n",
      "   MODALIDAD_VIOLENCIA_SEXUAL  eventos_especiales_mes  dias_festivos_mes  \n",
      "0                           0                       1                  1  \n",
      "1                           0                       1                  1  \n",
      "2                           0                       0                  1  \n",
      "3                           0                       0                  1  \n",
      "4                           0                       0                  1  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "\n",
      "Parámetros optimizados cargados:\n",
      "{'n_estimators': 50, 'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': True, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.8, 'num_leaves': 40, 'gamma': 0, 'metric': 'logloss', 'min_impurity_decrease': 0.1, 'feature_fraction': 0.6, 'min_child_samples': 10, 'min_weight_fraction_leaf': 0, 'objective': 'binary:logistic'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [11:25:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"feature_fraction\", \"metric\", \"min_child_samples\", \"min_impurity_decrease\", \"min_weight_fraction_leaf\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 906, number of negative: 1115\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000800 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 559\n",
      "[LightGBM] [Info] Number of data points in the train set: 2021, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448293 -> initscore=-0.207570\n",
      "[LightGBM] [Info] Start training from score -0.207570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [11:25:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"feature_fraction\", \"metric\", \"min_child_samples\", \"min_impurity_decrease\", \"min_weight_fraction_leaf\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [11:25:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"feature_fraction\", \"metric\", \"min_child_samples\", \"min_impurity_decrease\", \"min_weight_fraction_leaf\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 724, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000741 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 554\n",
      "[LightGBM] [Info] Number of data points in the train set: 1616, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448020 -> initscore=-0.208675\n",
      "[LightGBM] [Info] Start training from score -0.208675\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000434 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 553\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000745 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 558\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000624 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 552\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Info] Number of positive: 725, number of negative: 892\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000951 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 553\n",
      "[LightGBM] [Info] Number of data points in the train set: 1617, number of used features: 32\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.448361 -> initscore=-0.207294\n",
      "[LightGBM] [Info] Start training from score -0.207294\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n",
      "Métricas:\n",
      "\n",
      "Accuracy: 93.68%\n",
      "Precision: 93.83%\n",
      "Recall: 93.26%\n",
      "F1-score: 93.51%\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=0.8 will be ignored. Current value: feature_fraction=0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but ExtraTreesClassifier was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\klgt1\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but ExtraTreesClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 206\u001b[0m\n\u001b[0;32m    203\u001b[0m resultado \u001b[38;5;241m=\u001b[39m Resultado(parametros)\n\u001b[0;32m    204\u001b[0m resultado\u001b[38;5;241m.\u001b[39mupdate_metrics(accuracy, precision, recall, f1_score)\n\u001b[1;32m--> 206\u001b[0m \u001b[43mwrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpaint_confusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 134\u001b[0m, in \u001b[0;36mRandomForestClassifierWrapper.paint_confusion_matrix\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    131\u001b[0m conf_matrix \u001b[38;5;241m=\u001b[39m confusion_matrix(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_test, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict())\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# Visualizar la matriz de confusión\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m    135\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(conf_matrix, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlues\u001b[39m\u001b[38;5;124m'\u001b[39m, xticklabels\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y), yticklabels\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y))\n\u001b[0;32m    136\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPredicted labels\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, StackingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf, \n",
    "                 max_features, bootstrap, learning_rate, subsample, colsample_bytree, num_leaves, gamma,\n",
    "                 objective='binary:logistic', max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                 min_weight_fraction_leaf=0.0, min_child_samples=20, feature_fraction=0.6,  metric='logloss'):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.bootstrap = bootstrap\n",
    "        self.random_state = 42\n",
    "        self.stacking_model = None  \n",
    "        self.predictions = []\n",
    "        self.learning_rate = learning_rate  \n",
    "        self.subsample = subsample  \n",
    "        self.colsample_bytree = colsample_bytree  \n",
    "        self.num_leaves = num_leaves\n",
    "        self.gamma = gamma\n",
    "        self.objective = objective\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.min_child_samples = min_child_samples\n",
    "        self.feature_fraction = feature_fraction\n",
    "        self.metric = metric\n",
    "\n",
    "    def fit(self, X, y):        \n",
    "        estimators = [\n",
    "            ('xgb', XGBClassifier(n_estimators=self.n_estimators, max_depth=self.max_depth, random_state=self.random_state,\n",
    "                                  learning_rate=self.learning_rate, subsample=self.subsample, colsample_bytree=self.colsample_bytree,\n",
    "                                  gamma=self.gamma, objective=self.objective, max_leaf_nodes=self.max_leaf_nodes,\n",
    "                                  min_impurity_decrease=self.min_impurity_decrease, min_weight_fraction_leaf=self.min_weight_fraction_leaf,\n",
    "                                  min_child_samples=self.min_child_samples, feature_fraction=self.feature_fraction,metric=self.metric)),\n",
    "            \n",
    "            ('lgbm', LGBMClassifier(n_estimators=self.n_estimators, max_depth=self.max_depth, random_state=self.random_state,\n",
    "                                    learning_rate=self.learning_rate, subsample=self.subsample, colsample_bytree=self.colsample_bytree,\n",
    "                                    num_leaves=self.num_leaves, max_leaf_nodes=self.max_leaf_nodes,\n",
    "                                    min_child_samples=self.min_child_samples, feature_fraction=self.feature_fraction)),\n",
    "            \n",
    "            ('extra_trees', ExtraTreesClassifier(n_estimators=self.n_estimators, criterion=self.criterion,\n",
    "                                                 max_depth=self.max_depth, min_samples_split=self.min_samples_split,\n",
    "                                                 min_samples_leaf=self.min_samples_leaf, max_features=self.max_features,\n",
    "                                                 bootstrap=self.bootstrap, random_state=self.random_state))]\n",
    "            \n",
    "        # Definir el clasificador de nivel 2 (modelo base)\n",
    "        rf_base = RandomForestClassifier(n_estimators=self.n_estimators, random_state=self.random_state)\n",
    "        \n",
    "        # Crear el modelo de stacking\n",
    "        self.stacking_model = StackingClassifier(estimators=estimators, final_estimator=rf_base)\n",
    "        self.stacking_model.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.stacking_model.predict(X)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'n_estimators': self.n_estimators,\n",
    "            'criterion': self.criterion,\n",
    "            'max_depth': self.max_depth,\n",
    "            'min_samples_split': self.min_samples_split,\n",
    "            'min_samples_leaf': self.min_samples_leaf,\n",
    "            'max_features': self.max_features,\n",
    "            'bootstrap': self.bootstrap,\n",
    "            'learning_rate': self.learning_rate,\n",
    "            'subsample': self.subsample,\n",
    "            'colsample_bytree': self.colsample_bytree,\n",
    "            'num_leaves': self.num_leaves,\n",
    "            'gamma': self.gamma,\n",
    "            'objective': self.objective,\n",
    "            'max_leaf_nodes': self.max_leaf_nodes,\n",
    "            'min_impurity_decrease': self.min_impurity_decrease,\n",
    "            'min_weight_fraction_leaf': self.min_weight_fraction_leaf,\n",
    "            'min_child_samples': self.min_child_samples,\n",
    "            'feature_fraction': self.feature_fraction,\n",
    "            'metric': self.metric\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n",
    "\n",
    "\n",
    "class RandomForestClassifierWrapper:\n",
    "    def __init__(self, X, y, n_estimators, criterion, max_depth, min_samples_split,\n",
    "                 min_samples_leaf, max_features, bootstrap, learning_rate, subsample, colsample_bytree, num_leaves, gamma,\n",
    "                 objective='binary:logistic', max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                 min_weight_fraction_leaf=0.0, min_child_samples=20, feature_fraction=0.6, metric='logloss'):\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        self.X_train = self.X_train.values\n",
    "        self.y_train = self.y_train.values\n",
    "        \n",
    "        self.rf = RandomForest(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth,\n",
    "                                min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n",
    "                                max_features=max_features, bootstrap=bootstrap,\n",
    "                                learning_rate=learning_rate, subsample=subsample,\n",
    "                                colsample_bytree=colsample_bytree, num_leaves=num_leaves, gamma=gamma,\n",
    "                                objective=objective, max_leaf_nodes=max_leaf_nodes,\n",
    "                                min_impurity_decrease=min_impurity_decrease, min_weight_fraction_leaf=min_weight_fraction_leaf,\n",
    "                                min_child_samples=min_child_samples, feature_fraction=feature_fraction,metric = metric)\n",
    "\n",
    "    def train(self):\n",
    "        self.rf.fit(self.X_train, self.y_train)\n",
    "        \n",
    "    def predict(self):\n",
    "        return self.rf.predict(self.X_test)\n",
    "\n",
    "    def evaluate(self):\n",
    "        y_pred = self.predict()        \n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        precision = precision_score(self.y_test, y_pred, average='macro')\n",
    "        recall = recall_score(self.y_test, y_pred, average='macro')\n",
    "        f1 = f1_score(self.y_test, y_pred, average='macro')\n",
    "        return accuracy, precision, recall, f1\n",
    "\n",
    "    def paint_confusion_matrix(self):\n",
    "        # Calcular la matriz de confusión\n",
    "        conf_matrix = confusion_matrix(self.y_test, self.predict())\n",
    "        \n",
    "        # Visualizar la matriz de confusión\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(self.y_test), yticklabels=np.unique(self.y_test))\n",
    "        plt.xlabel('Predicted labels')\n",
    "        plt.ylabel('True labels')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class Resultado:\n",
    "    def __init__(self, parametros):\n",
    "        self.parametros = parametros\n",
    "        self.metricas = {'Accuracy': None, 'Precision': None, 'Recall': None, 'F1-score': None}\n",
    "\n",
    "    def update_metrics(self, accuracy, precision, recall, f1_score):\n",
    "        self.metricas['Accuracy'] = accuracy\n",
    "        self.metricas['Precision'] = precision\n",
    "        self.metricas['Recall'] = recall\n",
    "        self.metricas['F1-score'] = f1_score\n",
    "        print(\"Métricas:\\n\")\n",
    "        for metrica, valor in self.metricas.items():\n",
    "            print(f\"{metrica}: {valor * 100:.2f}%\")\n",
    "        print(\"\\n\\n\\n\")\n",
    "        \n",
    "    \n",
    "\n",
    "def loadBalancedDataSet(file_path):\n",
    "    \"\"\"\n",
    "    Carga un conjunto de datos balanceado desde un archivo Excel.   \n",
    "    Returns:\n",
    "    - X (DataFrame): Características del conjunto de datos.\n",
    "    - y (Series): Etiquetas del conjunto de datos.\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(file_path)\n",
    "    X = df.drop('CONDUCTA', axis=1)\n",
    "    y = df['CONDUCTA']\n",
    "    return X, y\n",
    "\n",
    "def loadOptimizedParameters(file_path):\n",
    "    \"\"\"\n",
    "    Carga los parámetros optimizados desde un archivo Excel.\n",
    "    Returns:\n",
    "    - parametros (dict): Parámetros optimizados.\n",
    "    \"\"\"\n",
    "    parametros_df = pd.read_excel(file_path)\n",
    "    fila = parametros_df.iloc[0]\n",
    "    parametros = dict(zip(parametros_df.columns, fila))\n",
    "    return parametros\n",
    "\n",
    "\n",
    "\n",
    "balanced_data_file = \"C:\\\\Users\\\\klgt1\\\\Downloads\\\\dataset_BALANCEADO.xlsx\"\n",
    "optimized_params_file = \"C:\\\\Users\\\\klgt1\\\\Downloads\\\\ParametrosOptimización.xlsx\"\n",
    "\n",
    "X, y = loadBalancedDataSet(balanced_data_file)\n",
    "\n",
    "parametros = loadOptimizedParameters(optimized_params_file)\n",
    "\n",
    "# Ejecutar el clasificador con los parámetros actuales\n",
    "wrapper = RandomForestClassifierWrapper(X, y, **parametros)\n",
    "wrapper.train()\n",
    "\n",
    "# Obtener los resultados de print_metrics\n",
    "accuracy, precision, recall, f1_score = wrapper.evaluate()\n",
    "\n",
    "# Crear un objeto Resultado y actualizar sus métricas\n",
    "resultado = Resultado(parametros)\n",
    "resultado.update_metrics(accuracy, precision, recall, f1_score)\n",
    "\n",
    "wrapper.paint_confusion_matrix()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
